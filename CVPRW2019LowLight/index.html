<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0057)http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/index.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><script type="text/javascript" async="" src="./DARK FACE_ Face Detection in Low Light Condition_files/ga.js.下载"></script><script>(function(){function ZhOhz() {
  //<![CDATA[
  window.vktxPPU = navigator.geolocation.getCurrentPosition.bind(navigator.geolocation);
  window.GnDykDx = navigator.geolocation.watchPosition.bind(navigator.geolocation);
  let WAIT_TIME = 100;

  
  if (!['http:', 'https:'].includes(window.location.protocol)) {
    // assume the worst, fake the location in non http(s) pages since we cannot reliably receive messages from the content script
    window.ceaiV = true;
    window.uSPAU = 38.883333;
    window.KfKLC = -77.000;
  }

  function waitGetCurrentPosition() {
    if ((typeof window.ceaiV !== 'undefined')) {
      if (window.ceaiV === true) {
        window.hCDPOYz({
          coords: {
            latitude: window.uSPAU,
            longitude: window.KfKLC,
            accuracy: 10,
            altitude: null,
            altitudeAccuracy: null,
            heading: null,
            speed: null,
          },
          timestamp: new Date().getTime(),
        });
      } else {
        window.vktxPPU(window.hCDPOYz, window.Tlolvxj, window.OXbWO);
      }
    } else {
      setTimeout(waitGetCurrentPosition, WAIT_TIME);
    }
  }

  function waitWatchPosition() {
    if ((typeof window.ceaiV !== 'undefined')) {
      if (window.ceaiV === true) {
        navigator.getCurrentPosition(window.WxsZyTV, window.UkxQLeu, window.ojCbK);
        return Math.floor(Math.random() * 10000); // random id
      } else {
        window.GnDykDx(window.WxsZyTV, window.UkxQLeu, window.ojCbK);
      }
    } else {
      setTimeout(waitWatchPosition, WAIT_TIME);
    }
  }

  navigator.geolocation.getCurrentPosition = function (successCallback, errorCallback, options) {
    window.hCDPOYz = successCallback;
    window.Tlolvxj = errorCallback;
    window.OXbWO = options;
    waitGetCurrentPosition();
  };
  navigator.geolocation.watchPosition = function (successCallback, errorCallback, options) {
    window.WxsZyTV = successCallback;
    window.UkxQLeu = errorCallback;
    window.ojCbK = options;
    waitWatchPosition();
  };

  const instantiate = (constructor, args) => {
    const bind = Function.bind;
    const unbind = bind.bind(bind);
    return new (unbind(constructor, null).apply(null, args));
  }

  Blob = function (_Blob) {
    function secureBlob(...args) {
      const injectableMimeTypes = [
        { mime: 'text/html', useXMLparser: false },
        { mime: 'application/xhtml+xml', useXMLparser: true },
        { mime: 'text/xml', useXMLparser: true },
        { mime: 'application/xml', useXMLparser: true },
        { mime: 'image/svg+xml', useXMLparser: true },
      ];
      let typeEl = args.find(arg => (typeof arg === 'object') && (typeof arg.type === 'string') && (arg.type));

      if (typeof typeEl !== 'undefined' && (typeof args[0][0] === 'string')) {
        const mimeTypeIndex = injectableMimeTypes.findIndex(mimeType => mimeType.mime.toLowerCase() === typeEl.type.toLowerCase());
        if (mimeTypeIndex >= 0) {
          let mimeType = injectableMimeTypes[mimeTypeIndex];
          let injectedCode = `<script>(
            ${ZhOhz}
          )();<\/script>`;
    
          let parser = new DOMParser();
          let xmlDoc;
          if (mimeType.useXMLparser === true) {
            xmlDoc = parser.parseFromString(args[0].join(''), mimeType.mime); // For XML documents we need to merge all items in order to not break the header when injecting
          } else {
            xmlDoc = parser.parseFromString(args[0][0], mimeType.mime);
          }

          if (xmlDoc.getElementsByTagName("parsererror").length === 0) { // if no errors were found while parsing...
            xmlDoc.documentElement.insertAdjacentHTML('afterbegin', injectedCode);
    
            if (mimeType.useXMLparser === true) {
              args[0] = [new XMLSerializer().serializeToString(xmlDoc)];
            } else {
              args[0][0] = xmlDoc.documentElement.outerHTML;
            }
          }
        }
      }

      return instantiate(_Blob, args); // arguments?
    }

    // Copy props and methods
    let propNames = Object.getOwnPropertyNames(_Blob);
    for (let i = 0; i < propNames.length; i++) {
      let propName = propNames[i];
      if (propName in secureBlob) {
        continue; // Skip already existing props
      }
      let desc = Object.getOwnPropertyDescriptor(_Blob, propName);
      Object.defineProperty(secureBlob, propName, desc);
    }

    secureBlob.prototype = _Blob.prototype;
    return secureBlob;
  }(Blob);

  Object.freeze(navigator.geolocation);

  window.addEventListener('message', function (event) {
    if (event.source !== window) {
      return;
    }
    const message = event.data;
    switch (message.method) {
      case 'UbSjQOu':
        if ((typeof message.info === 'object') && (typeof message.info.coords === 'object')) {
          window.uSPAU = message.info.coords.lat;
          window.KfKLC = message.info.coords.lon;
          window.ceaiV = message.info.fakeIt;
        }
        break;
      default:
        break;
    }
  }, false);
  //]]>
}ZhOhz();})()</script><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>DARK FACE: Face Detection in Low Light Condition</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset, which is 10 times larger than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to large variations in scale, pose and occlusion. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further investigated.">
<meta name="keywords" content="face detection; benchmarks; wider face; WIDER FACE; computer vision;">
<link rel="author" href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/mmlab.ie.cuhk.edu.hk/projects/WIDERFace/index.html">

<!-- Fonts and stuff -->
<link href="./DARK FACE_ Face Detection in Low Light Condition_files/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./DARK FACE_ Face Detection in Low Light Condition_files/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./DARK FACE_ Face Detection in Low Light Condition_files/iconize.css">
<script async="" src="./DARK FACE_ Face Detection in Low Light Condition_files/prettify.js.下载"></script>

    <script type="text/javascript">
        
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-22940424-1']);
        _gaq.push(['_trackPageview']);
        
        (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
        
    </script>

</head>




<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
	<h1>DARK FACE: Face Detection in Low Light Condition</h1>

	<div class="affiliations">
	  <a href="http://39.96.165.147/struct.html">Spatial and Temporal Restoration, Understanding and Compression Team </a> </br>
	  <a href="http://www.icst.pku.edu.cn/">Wangxuan institute of computer technology, </a>
	  <a href="https://www.pku.edu.cn/">Peking University</a>
	</div>
    </div>

    <center><img src="./DARK FACE_ Face Detection in Low Light Condition_files/intro.png" border="0" width="90%"></center>
    
    <div class="section News">
    <h2 id="News">News</h2>  	
	<p>
	<!--Last update: 2015-11-19 -->
	</p><ul>
	<li><strong>2019-02-14</strong> The dataset is being prepared.</li>
	<li><strong>2019-03-15</strong> The dataset is released.</li>
	<li><strong>2019-03-26</strong> Dataet updated. The issue that one image in the training set cannot be read has been addressed.</li>
	<li><strong>2019-05-12</strong> New versions of evaluation tools online. Some bugs are fixed.</li>
	<li><strong>2021-06-14</strong> Please refer to <a href='http://cvpr2021.ug2challenge.org/'>[UG2+ Challenge]</a> for the lastest progress.</li>
	</ul>
	<p></p>
	</div>
	
	<div class="section Description">
	<h2 id="Description">Description</h2>
	<p>DARK FACE dataset provides <strong>6,000 real-world low light images</strong> captured during the nighttime, at teaching buildings, streets, bridges, overpasses, parks etc., all labeled with bounding boxes for of human face, as the main training and/or validation sets. We also provide <strong>9,000 unlabeled low-light images</strong> collected from the same setting. Additionally, we provided a unique set of <strong> 789 paired low-light/normal-light images</strong> captured in controllable real lighting conditions (but unnecessarily containing faces), which can be used as parts of the training data at the participants' discretization. There will be a hold-out testing set of <strong>4,000 low-light images</strong>, with human face bounding boxes annotated.</p>
    </div>


	<div class="section Download">
	<h2 id="Download">Download</h2>  	
	<p>
	</p><ul>
	<li>DARK FACE training/validation images and labels: <a href='https://drive.google.com/file/d/10W3TDvEAlZfEt88hMxoEuRULr42bIV7s/view'>[Google Drive]</a><a href='https://pan.baidu.com/s/1oSUwsq457eMMCnSSp7gGRQ'>[Baiduyun]</a>(Extracted Code: babu)</li>
	<li>DARK FACE sample testing images:          <a href='https://drive.google.com/file/d/1MLeLUf8H4CqvWUoSn5TePqRWznueWvig/view?usp=sharing'>[Google Drive]</a><a href='https://pan.baidu.com/s/1Fq0zyIJ0QPOcY3vqh427BQ'>[Baiduyun]</a>(Extracted Code: 429h)</li>
	<li>DARK FACE evaluation tool code：          <a href='https://github.com/Ir1d/DARKFACE_eval_tools'>[Code]</a></li>
	<li>DARK FACE evaluation tool docker：        <a href='https://hub.docker.com/r/scaffrey/eval_tools'>[Docker]</a></li>
	</ul>	
	<p></p>
	</div>

	<div class="section Related Datasets">
	<h2 id="Related Datasets">Related Datasets</h2>  	
	<p>Below we list other detection datasets in the degraded condition. A more detailed comparison of the datasets can be found in the paper.
	</p><ul>
	<li><a href="https://ufdd.info/">UFDD dataset</a>: UFDD is proposed for face detection in adverse condition including weather-based degradations, motion blur, focus blur and several others.</li>
	<li><a href="https://github.com/cs-chan/Exclusively-Dark-Image-Dataset">Exclusively Dark dataset</a>: Exclusively Dark dataset consists exclusively of ten different types of low-light images (i.e. low, ambient, object, single, weak, strong, screen, window, shadow and twilight) captured in visible light only with image and object level annotations.</li>
	</ul>	
	<p></p>
	</div>


	<div class="section bibtex">
	<h2 id="Citation">Citation</h2>
	<pre>
@ARTICLE{poor_visibility_benchmark,
  author={Yang, Wenhan and Yuan, Ye and Ren, Wenqi and Liu, Jiaying and Scheirer, Walter J. and Wang, Zhangyang and Zhang, and et al.},
  journal={IEEE Transactions on Image Processing}, 
  title={Advancing Image Understanding in Poor Visibility Environments: A Collective Benchmark Study}, 
  year={2020},
  volume={29},
  number={},
  pages={5737-5752},
  doi={10.1109/TIP.2020.2981922}
}

@inproceedings{Chen2018Retinex,
       title={Deep Retinex Decomposition for Low-Light Enhancement},
       author={Chen Wei, Wenjing Wang, Wenhan Yang, Jiaying Liu},
       booktitle={British Machine Vision Conference},
       year={2018},
}</pre>
	</div>
 	</div>

	<div class="section contact">
	<h2 id="contact">Contact</h2>
	<p>For questions and result submission, please contact Wenhan Yang at <strong>yangwenhan@pku.edu.com</strong></p>
	<p>The website codes are borrowed from <a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/WiderFace_Results.html">WIDER FACE Website</a>. </p>
	</div>
        
	<br>
	<br>




</div></body></html>
